{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T13:52:59.716819Z",
     "start_time": "2018-04-09T13:52:59.352454Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Dynamic Routing Between Capsules\n",
    "# https://arxiv.org/pdf/1710.09829.pdf\n",
    "#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from capsule_network import CapsuleNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T13:52:59.723816Z",
     "start_time": "2018-04-09T13:52:59.718597Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Settings.\n",
    "#\n",
    "learning_rate = 0.01\n",
    "\n",
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "# Stop training if loss goes below this threshold.\n",
    "early_stop_loss = 0.0001\n",
    "\n",
    "# printer interval\n",
    "log_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T13:53:07.190361Z",
     "start_time": "2018-04-09T13:52:59.726032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapsuleNetwork(\n",
      "  (conv1): CapsuleConvLayer(\n",
      "    (conv0): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
      "    (relu): ReLU(inplace)\n",
      "  )\n",
      "  (primary): CapsuleLayer(\n",
      "    (unit_0): ConvUnit(\n",
      "      (conv0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "    )\n",
      "    (unit_1): ConvUnit(\n",
      "      (conv0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "    )\n",
      "    (unit_2): ConvUnit(\n",
      "      (conv0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "    )\n",
      "    (unit_3): ConvUnit(\n",
      "      (conv0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "    )\n",
      "    (unit_4): ConvUnit(\n",
      "      (conv0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "    )\n",
      "    (unit_5): ConvUnit(\n",
      "      (conv0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "    )\n",
      "    (unit_6): ConvUnit(\n",
      "      (conv0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "    )\n",
      "    (unit_7): ConvUnit(\n",
      "      (conv0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (digits): CapsuleLayer(\n",
      "  )\n",
      "  (reconstruct): CapsuleReconstrutionLayer(\n",
      "    (reconstruct0): Linear(in_features=160, out_features=522, bias=True)\n",
      "    (reconstruct1): Linear(in_features=522, out_features=1176, bias=True)\n",
      "    (reconstruct2): Linear(in_features=1176, out_features=784, bias=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load MNIST dataset.\n",
    "#\n",
    "\n",
    "# Normalization for MNIST dataset.\n",
    "dataset_transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=dataset_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('../data', train=False, download=True, transform=dataset_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "#\n",
    "# Create capsule network.\n",
    "#\n",
    "\n",
    "conv_inputs = 1\n",
    "conv_outputs = 256\n",
    "num_primary_units = 8\n",
    "primary_unit_size = 32 * 6 * 6  # fixme get from conv2d\n",
    "output_unit_size = 16\n",
    "\n",
    "network = CapsuleNetwork(image_width=28,\n",
    "                         image_height=28,\n",
    "                         image_channels=1,\n",
    "                         conv_inputs=conv_inputs,\n",
    "                         conv_outputs=conv_outputs,\n",
    "                         num_primary_units=num_primary_units,\n",
    "                         primary_unit_size=primary_unit_size,\n",
    "                         num_output_units=10, # one for each MNIST digit\n",
    "                         output_unit_size=output_unit_size).cuda()\n",
    "print(network)\n",
    "\n",
    "\n",
    "# Converts batches of class indices to classes of one-hot vectors.\n",
    "def to_one_hot(x, length):\n",
    "    batch_size = x.size(0)\n",
    "    x_one_hot = torch.zeros(batch_size, length)\n",
    "    for i in range(batch_size):\n",
    "        x_one_hot[i, x[i]] = 1.0\n",
    "    return x_one_hot\n",
    "\n",
    "# This is the test function from the basic Pytorch MNIST example, but adapted to use the capsule network.\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target_indices = target\n",
    "        target_one_hot = to_one_hot(target_indices, length=network.digits.num_units)\n",
    "\n",
    "        data, target = Variable(data, volatile=True).cuda(), Variable(target_one_hot).cuda()\n",
    "\n",
    "        output = network(data)\n",
    "\n",
    "        test_loss += network.loss(data, output, target, size_average=False).data[0] # sum up batch loss\n",
    "\n",
    "        v_mag = torch.sqrt((output**2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        pred = v_mag.data.max(1, keepdim=True)[1].cpu()\n",
    "\n",
    "        correct += pred.eq(target_indices.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss,\n",
    "        correct,\n",
    "        len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "# This is the train function from the basic Pytorch MNIST example, but adapted to use the capsule network.\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "def train(epoch):\n",
    "    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "    last_loss = None\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target_one_hot = to_one_hot(target, length=network.digits.num_units)\n",
    "\n",
    "        data, target = Variable(data).cuda(), Variable(target_one_hot).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = network(data)\n",
    "\n",
    "        loss = network.loss(data, output, target)\n",
    "        loss.backward()\n",
    "        last_loss = loss.data[0]\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data),\n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0]))\n",
    "\n",
    "        if last_loss < early_stop_loss:\n",
    "            break\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T14:26:08.965675Z",
     "start_time": "2018-04-09T13:53:07.194905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/workspace/pytorch-capsule/capsule_layer.py:102: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  c_ij = F.softmax(b_ij)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.299764\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.104034\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.080433\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.958816\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.848023\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.801446\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.785073\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.746291\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.741426\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.701898\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.654262\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.618312\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.662694\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.582815\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.586030\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.552906\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.527277\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.495819\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.500309\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.456931\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.463156\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.430891\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.436933\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.408578\n",
      "\n",
      "Test set: Average loss: 0.0033, Accuracy: 9434/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.409981\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.425058\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.390609\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.373927\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.380959\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.398504\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.377994\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.363119\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.353288\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.354369\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.338527\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.356964\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.315426\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.356474\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.315984\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.344859\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.338378\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.323332\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.336142\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.336428\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.316235\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.308448\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.313619\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.310634\n",
      "\n",
      "Test set: Average loss: 0.0025, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.326203\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.300453\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.312360\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.323978\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.296887\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.310772\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.309339\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.280316\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.288416\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.289304\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.296670\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.309463\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.307428\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.291470\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.290137\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.284571\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.275736\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.297903\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.293332\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.276880\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.277867\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.297683\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.293848\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.291935\n",
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.278609\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.290615\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.289248\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.276293\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.315478\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.280115\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.267215\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.292073\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.269000\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.294704\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.288192\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.273525\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.280903\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.275766\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.273654\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.276332\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.291341\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.275914\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.286869\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.266738\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.285972\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.272700\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.270042\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.277416\n",
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 9863/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.265481\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.267870\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.281562\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.265273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c14fdf059b4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlast_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlast_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mearly_stop_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4ee66c6b45e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mlast_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    last_loss = train(epoch)\n",
    "    test()\n",
    "    if last_loss < early_stop_loss:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
